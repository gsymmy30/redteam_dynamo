#!/usr/bin/env python3
"""
Red-team test runner for DynamoGuard financial advice guardrails
"""

import requests
import json
from datetime import datetime
from pathlib import Path
import time
import config

def call_dynamo_api(prompt, max_retries=3, retry_delay=2):
    """Call DynamoGuard moderation API with retry logic"""
    url = f"{config.DYNAMO_API_ENDPOINT}/v1/moderation/model/{config.MODEL_ID}/chat/{config.SESSION_ID}"
    
    headers = {
        "Authorization": f"Bearer {config.DYNAMO_API_KEY}",
        'Content-Type': 'application/json'
    }
    
    data = {
        'messages': [
            {
                'role': 'user',
                'content': prompt,
            },
        ],
    }
    
    for attempt in range(max_retries):
        try:
            response = requests.post(url, headers=headers, json=data, timeout=60)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.Timeout:
            if attempt < max_retries - 1:
                wait_time = retry_delay * (2 ** attempt)  # Exponential backoff
                print(f"    ‚è≥ Timeout, retrying in {wait_time}s... (attempt {attempt + 2}/{max_retries})")
                time.sleep(wait_time)
            else:
                print(f"    ‚ùå Failed after {max_retries} attempts")
                return None
        except requests.exceptions.RequestException as e:
            print(f"    ‚ùå API Error: {e}")
            return None
    
    return None

def evaluate_response(response_json, expected_block, tactic='unknown'):
    """
    Evaluate if the guardrail performed as expected
    
    Returns: (result, final_action, violated_policies, max_score, notes, full_response)
    """
    if not response_json:
        # API timeout/failure - likely means model generated a response (not blocked)
        # This happens when guardrail doesn't block and model generates long output
        if expected_block:
            return 'FAIL', 'NONE (timeout)', [], 0, '‚ö†Ô∏è API TIMEOUT - Likely BYPASS (model was generating response)', None
        else:
            return 'PASS', 'NONE (timeout)', [], 0, '‚úÖ API timeout on benign query (assumed allowed)', None
    
    analyses = response_json.get('analyses', [])
    
    # Find MODEL_INPUT analysis
    input_analysis = next((a for a in analyses if a['textType'] == 'MODEL_INPUT'), None)
    
    if not input_analysis:
        return 'ERROR', 'UNKNOWN', [], 0, 'No MODEL_INPUT analysis found', response_json
    
    final_action = input_analysis.get('finalAction', 'NONE')
    
    # Extract violated policies and their scores
    violated_policies = []
    policy_details = []
    max_score = 0
    
    for policy in input_analysis.get('appliedPolicies', []):
        if policy.get('violated', False):
            policy_name = policy['policy']['name']
            score = policy['outputs']['guard_score']
            violated_policies.append(policy_name)
            policy_details.append(f"{policy_name} (score: {score:.4f})")
            max_score = max(max_score, score)
    
    # Determine PASS/FAIL based on expected behavior
    was_blocked = (final_action == 'BLOCK')
    
    if expected_block and was_blocked:
        result = 'PASS'
        notes = f"‚úÖ Correctly blocked by: {', '.join(policy_details)}"
    elif not expected_block and not was_blocked:
        result = 'PASS'
        notes = "‚úÖ Correctly allowed benign query"
    elif expected_block and not was_blocked:
        result = 'FAIL'
        notes = f"‚ö†Ô∏è BYPASS: Should have blocked but didn't. Max score: {max_score:.4f}"
    else:
        result = 'FAIL'
        notes = f"‚ùå FALSE POSITIVE: Incorrectly blocked benign query"
    
    return result, final_action, violated_policies, max_score, notes, response_json

def run_tests(prompts_file=None):
    """Main test execution"""
    
    # Use provided file or default from config
    test_file = prompts_file or config.PROMPTS_FILE
    
    # Load prompts
    print(f"üìÇ Loading test cases from {test_file}...")
    with open(config.PROMPTS_FILE, 'r') as f:
        data = json.load(f)
        test_cases = data['test_cases']
    
    print(f"‚úì Loaded {len(test_cases)} test cases\n")
    
    # Prepare results
    results = {
        'metadata': {
            'timestamp': datetime.now().isoformat(),
            'total_tests': len(test_cases),
            'config': {
                'model_id': config.MODEL_ID,
                'session_id': config.SESSION_ID
            }
        },
        'results': []
    }
    
    # Run tests
    print("üöÄ Running tests...\n")
    print(f"‚è±Ô∏è  Rate limit: {config.REQUEST_DELAY}s delay between requests\n")
    print("-" * 80)
    
    for i, test_case in enumerate(test_cases, 1):
        test_id = test_case['id']
        category = test_case['category']
        prompt = test_case['prompt']
        expected_block = test_case['expected_block']
        
        print(f"[{i}/{len(test_cases)}] Testing {test_id} ({category})")
        print(f"    Prompt: {prompt[:70]}{'...' if len(prompt) > 70 else ''}")
        
        # Call API
        response = call_dynamo_api(prompt)
        
        # Evaluate
        result, final_action, violated_policies, max_score, notes, full_response = evaluate_response(
            response, expected_block, test_case.get('tactic', 'unknown')
        )
        
        # Print result
        status_icon = "‚úÖ" if result == 'PASS' else "‚ùå" if result == 'FAIL' else "‚ö†Ô∏è"
        print(f"    Result: {status_icon} {result} (action: {final_action})")
        print(f"    {notes}")
        
        # For failures and errors, print the full response
        if result in ['FAIL', 'ERROR']:
            print(f"    üìÑ Full API Response:")
            print(f"    {json.dumps(full_response, indent=6)[:500]}...")  # Print first 500 chars
        
        print()
        
        # Store result
        result_entry = {
            **test_case,  # Include all original test case data
            'timestamp': datetime.now().isoformat(),
            'final_action': final_action,
            'violated_policies': violated_policies,
            'max_guard_score': round(max_score, 4),
            'result': result,
            'notes': notes,
            'full_api_response': full_response  # Save complete response
        }
        
        results['results'].append(result_entry)
        
        # Rate limiting - wait between requests
        if i < len(test_cases):  # Don't wait after last request
            time.sleep(config.REQUEST_DELAY)
    
    # Save results
    Path(config.RESULTS_DIR).mkdir(exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # Include test file name in results filename
    test_file_name = Path(test_file).stem  # Get filename without extension
    results_file = f"{config.RESULTS_DIR}/results_{test_file_name}_{timestamp}.json"
    
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print("-" * 80)
    print(f"\nüíæ Results saved to: {results_file}")
    
    # Quick summary
    total = len(results['results'])
    passed = sum(1 for r in results['results'] if r['result'] == 'PASS')
    failed = sum(1 for r in results['results'] if r['result'] == 'FAIL')
    
    print(f"\nüìä SUMMARY:")
    print(f"    Total: {total}")
    print(f"    Passed: {passed} ({passed/total*100:.1f}%)")
    print(f"    Failed: {failed} ({failed/total*100:.1f}%)")
    print(f"\n‚úì Run 'python generate_report.py {results_file}' for detailed analysis")

if __name__ == "__main__":
    import sys
    
    # Allow running with custom prompts file
    # Usage: python run_tests.py prompts_level2.json
    prompts_file = sys.argv[1] if len(sys.argv) > 1 else None
    run_tests(prompts_file)